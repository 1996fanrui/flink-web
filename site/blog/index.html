<!DOCTYPE html>
<html lang="en">
    <head>
	    <meta charset="utf-8">
	    <meta http-equiv="X-UA-Compatible" content="IE=edge">
	    <meta name="viewport" content="width=device-width, initial-scale=1">

	    <title>Apache Flink: Blog</title>
	    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
	    <link rel="icon" href="favicon.ico" type="image/x-icon">
	    <link rel="stylesheet" href="/css/bootstrap.css">
	    <link rel="stylesheet" href="/css/bootstrap-lumen-custom.css">
	    <link rel="stylesheet" href="/css/syntax.css">
	    <link rel="stylesheet" href="/css/custom.css">
	    <link href="/css/main/main.css" rel="stylesheet">
            <link href="/blog/feed.xml" rel="alternate" type="application/rss+xml" title="Flink Blog RSS feed" />
	    <!-- <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet"> -->
	    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
	    <script src="/js/bootstrap.min.js"></script>
    </head>
    <body>
    <div class="af-header-container af-inner-pages-navigation">
	<header>
		<div class="container">
			<div class="row">
				<div class="col-md-1 af-mobile-nav-bar">
					<a href="/" title="Home">
					<img class="hidden-xs hidden-sm img-responsive"
						src="/img/main/logo.png" alt="Apache Flink Logo">
					</a>	
					<div class="row visible-xs">
						<div class="col-xs-3">
						    <a href="/" title="Home">  
							<img class="hidden-x hidden-sm img-responsive"
								src="/img/main/logo.png" alt="Apache Flink Logo">
							</a>	
						</div>
						<div class="col-xs-5"></div>
						<div class="col-xs-4">
							<div class="af-mobile-btn">
								<span class="glyphicon glyphicon-plus"></span>
							</div>
						</div>
					</div>
				</div>
				<!-- Navigation -->
				<div class="col-md-11">
					<nav class="af-main-nav" role="navigation">
						<ul>
							<li><a href="#" class="af-nav-links">Quickstart
									<b class="caret"></b>
							</a>
								<ul class="af-dropdown-menu">
									<li><a href="/docs/0.8/setup_quickstart.html">Setup
											Flink</a></li>
									<li><a
										href="/docs/0.8/java_api_quickstart.html">Java
											API</a></li>
									<li><a
										href="/docs/0.8/scala_api_quickstart.html">Scala
											API</a></li>
								</ul></li>
							<li><a href="/downloads.html">Download</a></li>
							<li><a href="/docs/0.8/faq.html">FAQ</a></li>
							<li><a href="#" class="af-nav-links">Documentation <b
									class="caret"></b></a>
								<ul class="af-dropdown-menu">
									<li class="af-separator">Current Stable:</li>
									<li></li>
									<li><a href="/docs/0.8/">0.8.0</a></li>
									<li><a href="/docs/0.8/api/java">0.8.0 Javadocs</a></li>
									<li><a
										href="/docs/0.8/api/scala/index.html#org.apache.flink.api.scala.package">0.8.0 Scaladocs</a></li>
									<li class="divider"></li>
									<li class="af-separator">Previous:</li>
									<li></li>
									<li><a href="/docs/0.7-incubating/">0.7.0-incubating</a></li>
									<li><a href="/docs/0.7-incubating/api/java">0.7.0-incubating
											Javadocs</a></li>
									<li><a
										href="/docs/0.7-incubating/api/scala/index.html#org.apache.flink.api.scala.package">0.7.0-incubating
											Scaladocs</a></li>
									<li class="divider"></li>
									<li></li>
									<li><a href="/docs/0.6-incubating/">0.6-incubating</a></li>
									<li><a href="/docs/0.6-incubating/api/java">0.6-incubating
											Javadocs</a></li>

								</ul></li>
							<li><a href="#" class="af-nav-links">Community <b
									class="caret"></b></a>
								<ul class="af-dropdown-menu">
									<li><a href="/community.html#mailing-lists">Mailing
											Lists</a></li>
									<li><a href="/community.html#issues">Issues</a></li>
									<li><a href="/community.html#team">Team</a></li>
									<li class="divider"></li>
									<li><a href="/how-to-contribute.html">How To
											Contribute</a></li>
									<li><a href="/coding_guidelines.html">Coding
											Guidelines</a></li>
								</ul></li>
							<li><a href="#" class="af-nav-links">Project <b
									class="caret"></b></a>
								<ul class="af-dropdown-menu">
									<li><a href="/material.html">Material</a></li>
									<li><a href="http://www.apache.org/">Apache Software
											Foundation <span class="glyphicon glyphicon-new-window"></span>
									</a></li>
									<li><a
										href="https://cwiki.apache.org/confluence/display/FLINK">Wiki
											<span class="glyphicon glyphicon-new-window"></span>
									</a></li>
									<li><a
										href="https://wiki.apache.org/incubator/StratosphereProposal">Incubator
											Proposal <span class="glyphicon glyphicon-new-window"></span>
									</a></li>
									<li><a href="http://www.apache.org/licenses/LICENSE-2.0">License
											<span class="glyphicon glyphicon-new-window"></span>
									</a></li>
									<li><a href="https://github.com/apache/incubator-flink">Source
											Code <span class="glyphicon glyphicon-new-window"></span>
									</a></li>
								</ul></li>
							<li><a href="/blog/index.html" class="">Blog</a></li>
						</ul>
					</nav>
				</div>
			</div>
		</div>
	</header>
</div>


    <div style="padding-top:120px" class="container">
        <div class="container">
	<div class="row">
		<div class="col-md-2"></div>
		<div class="col-md-8">
			
			<article>
				<h2><a href="/news/2015/02/04/january-in-flink.html">January 2015 in the Flink community</a></h2>
				<p class="meta">04 Feb 2015</p>

				<div><p>Happy 2015! Here is a (hopefully digestible) summary of what happened last month in the Flink community.</p>

<h3 id="0.8.0-release">0.8.0 release</h3>

<p>Flink 0.8.0 was released. See <a href="http://flink.apache.org/news/2015/01/21/release-0.8.html">here</a> for the release notes.</p>

<h3 id="flink-roadmap">Flink roadmap</h3>

<p>The community has published a <a href="https://cwiki.apache.org/confluence/display/FLINK/Flink+Roadmap">roadmap for 2015</a> on the Flink wiki. Check it out to see what is coming up in Flink, and pick up an issue to contribute!</p>

<h3 id="scaling-als">Scaling ALS</h3>

<p>Flink committers employed at <a href="http://data-artisans.com">data Artisans</a> published a <a href="http://data-artisans.com/computing-recommendations-with-flink.html">blog post</a> on how they scaled matrix factorization with Flink and Google Compute Engine to matrices with 28 billion elements.</p>

<h3 id="articles-in-the-press">Articles in the press</h3>

<p>The Apache Software Foundation <a href="https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces69">announced</a> Flink as a Top-Level Project. The announcement was picked up by the media, e.g., <a href="http://sdtimes.com/inside-apache-software-foundations-newest-top-level-project-apache-flink/?utm_content=11232092&amp;utm_medium=social&amp;utm_source=twitter">here</a>, <a href="http://www.datanami.com/2015/01/12/apache-flink-takes-route-distributed-data-processing/">here</a>, and <a href="http://i-programmer.info/news/197-data-mining/8176-flink-reaches-top-level-status.html">here</a>.</p>

<h3 id="hadoop-summit">Hadoop Summit</h3>

<p>A submitted abstract on Flink Streaming <a href="http://2015.hadoopsummit.org/amsterdam-blog/announcing-the-community-vote-session-winners-for-the-2015-hadoop-summit-europe/">won the community</a> vote at “The Future of Hadoop” track.</p>

<h3 id="meetups-and-talks">Meetups and talks</h3>

<p>Flink was presented at the <a href="http://www.meetup.com/Hadoop-User-Group-France/events/219778022/">Paris Hadoop User Group</a>, the <a href="http://www.meetup.com/hadoop/events/167785202/">Bay Area Hadoop User Group</a>, the <a href="http://www.meetup.com/Apache-Tez-User-Group/events/219302692/">Apache Tez User Group</a>, and <a href="https://fosdem.org/2015/schedule/track/graph_processing/">FOSDEM 2015</a>. The January <a href="http://www.meetup.com/Apache-Flink-Meetup/events/219639984/">Flink meetup in Berlin</a> had talks on recent community updates and new features.</p>

<h2 id="notable-code-contributions">Notable code contributions</h2>

<p><strong>Note:</strong> Code contributions listed here may not be part of a release or even the Flink master repository yet.</p>

<h3 id="using-off-heap-memory"><a href="https://github.com/apache/flink/pull/290">Using off-heap memory</a></h3>

<p>This pull request enables Flink to use off-heap memory for its internal memory uses (sort, hash, caching of intermediate data sets). </p>

<h3 id="gelly,-flink’s-graph-api"><a href="https://github.com/apache/flink/pull/335">Gelly, Flink’s Graph API</a></h3>

<p>This pull request introduces Gelly, Flink’s brand new Graph API. Gelly offers a native graph programming abstraction with functionality for vertex-centric programming, as well as available graph algorithms. See <a href="http://www.slideshare.net/vkalavri/largescale-graph-processing-with-apache-flink-graphdevroom-fosdem15">this slide set</a> for an overview of Gelly.</p>

<h3 id="semantic-annotations"><a href="https://github.com/apache/flink/pull/311">Semantic annotations</a></h3>

<p>Semantic annotations are a powerful mechanism to expose information about the behavior of Flink functions to Flink’s optimizer. The optimizer can leverage this information to generate more efficient execution plans. For example the output of a Reduce operator that groups on the second field of a tuple is still partitioned on that field if the Reduce function does not modify the value of the second field. By exposing this information to the optimizer, the optimizer can generate plans that avoid expensive data shuffling and reuse the partitioned output of Reduce. Semantic annotations can be defined for most data types, including (nested) tuples and POJOs. See the snapshot documentation for details (not online yet).</p>

<h3 id="new-yarn-client"><a href="https://github.com/apache/flink/pull/292">New YARN client</a></h3>

<p>The improved YARN client of Flink now allows users to deploy Flink on YARN for executing a single job. Older versions only supported a long-running YARN session. The code of the YARN client has been refactored to provide an (internal) Java API for controlling YARN clusters more easily.</p>
</div>
				<a href="/news/2015/02/04/january-in-flink.html#disqus_thread">January 2015 in the Flink community</a>
			</article>
			
			<article>
				<h2><a href="/news/2015/01/21/release-0.8.html">Apache Flink 0.8.0 available</a></h2>
				<p class="meta">21 Jan 2015</p>

				<div><p>We are pleased to announce the availability of Flink 0.8.0. This release includes new user-facing features as well as performance and bug fixes, extends the support for filesystems and introduces the Scala API and flexible windowing semantics for Flink Streaming. A total of 33 people have contributed to this release, a big thanks to all of them!</p>

<p><a href="http://www.apache.org/dyn/closer.cgi/flink/flink-0.8.0/flink-0.8.0-bin-hadoop2.tgz">Download Flink 0.8.0</a></p>

<p><a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12328699">See the release changelog</a></p>

<h2 id="overview-of-major-new-features">Overview of major new features</h2>

<ul>
<li><p><strong>Extended filesystem support</strong>: The former <code>DistributedFileSystem</code> interface has been generalized to <code>HadoopFileSystem</code> now supporting all sub classes of <code>org.apache.hadoop.fs.FileSystem</code>. This allows users to use all file systems supported by Hadoop with Apache Flink.
<a href="http://flink.incubator.apache.org/docs/0.8/example_connectors.html">See connecting to other systems</a></p></li>
<li><p><strong>Streaming Scala API</strong>: As an alternative to the existing Java API Streaming is now also programmable in Scala. The Java and Scala APIs have now the same syntax and transformations and will be kept from now on in sync in every future release.</p></li>
<li><p><strong>Streaming windowing semantics</strong>: The new windowing api offers an expressive way to define custom logic for triggering the execution of a stream window and removing elements. The new features include out-of-the-box support for windows based in logical or physical time and data-driven properties on the events themselves among others. <a href="http://flink.apache.org/docs/0.8/streaming_guide.html#window-operators">Read more here</a></p></li>
<li><p><strong>Mutable and immutable objects in runtime</strong> All Flink versions before 0.8.0 were always passing the same objects to functions written by users. This is a common performance optimization, also used in other systems such as Hadoop.
However, this is error-prone for new users because one has to carefully check that references to the object aren’t kept in the user function. Starting from 0.8.0, Flink allows to configure a mode which is disabling that mechanism.</p></li>
<li><p><strong>Performance and usability improvements</strong>: The new Apache Flink 0.8.0 release brings several new features which will significantly improve the performance and the usability of the system. Amongst others, these features include:</p>

<ul>
<li>Improved input split assignment which maximizes computation locality</li>
<li>Smart broadcasting mechanism which minimizes network I/O</li>
<li>Custom partitioners which let the user control how the data is partitioned within the cluster. This helps to prevent data skewness and allows to implement highly efficient algorithms.</li>
<li>coGroup operator now supports group sorting for its inputs</li>
</ul></li>
<li><p><strong>Kryo is the new fallback serializer</strong>: Apache Flink has a sophisticated type analysis and serialization framework that is able to handle commonly used types very efficiently.
In addition to that, there is a fallback serializer for types which are not supported. Older versions of Flink used the reflective <a href="http://avro.apache.org/">Avro</a> serializer for that purpose. With this release, Flink is using the powerful <a href="https://github.com/EsotericSoftware/kryo">Kryo</a> and twitter-chill library for support of types such as Java Collections and Scala specifc types.</p></li>
<li><p><strong>Hadoop 2.2.0+ is now the default Hadoop dependency</strong>: With Flink 0.8.0 we made the “hadoop2” build profile the default build for Flink. This means that all users using Hadoop 1 (0.2X or 1.2.X versions) have to specify  version “0.8.0-hadoop1” in their pom files.</p></li>
<li><p><strong>HBase module updated</strong> The HBase version has been updated to 0.98.6.1. Also, Hbase is now available to the Hadoop1 and Hadoop2 profile of Flink.</p></li>
</ul>

<h2 id="contributors">Contributors</h2>

<ul>
<li>Marton Balassi</li>
<li>Daniel Bali</li>
<li>Carsten Brandt</li>
<li>Moritz Borgmann</li>
<li>Stefan Bunk</li>
<li>Paris Carbone</li>
<li>Ufuk Celebi</li>
<li>Nils Engelbach </li>
<li>Stephan Ewen</li>
<li>Gyula Fora</li>
<li>Gabor Hermann</li>
<li>Fabian Hueske</li>
<li>Vasiliki Kalavri</li>
<li>Johannes Kirschnick</li>
<li>Aljoscha Krettek</li>
<li>Suneel Marthi</li>
<li>Robert Metzger</li>
<li>Felix Neutatz</li>
<li>Chiwan Park</li>
<li>Flavio Pompermaier</li>
<li>Mingliang Qi</li>
<li>Shiva Teja Reddy</li>
<li>Till Rohrmann</li>
<li>Henry Saputra</li>
<li>Kousuke Saruta</li>
<li>Chesney Schepler</li>
<li>Erich Schubert</li>
<li>Peter Szabo</li>
<li>Jonas Traub</li>
<li>Kostas Tzoumas</li>
<li>Timo Walther</li>
<li>Daniel Warneke</li>
<li>Chen Xu</li>
</ul>
</div>
				<a href="/news/2015/01/21/release-0.8.html#disqus_thread">Apache Flink 0.8.0 available</a>
			</article>
			
			<article>
				<h2><a href="/news/2015/01/06/december-in-flink.html">December 2014 in the Flink community</a></h2>
				<p class="meta">06 Jan 2015</p>

				<div><p>This is the first blog post of a “newsletter” like series where we give a summary of the monthly activity in the Flink community. As the Flink project grows, this can serve as a &quot;tl;dr&quot; for people that are not following the Flink dev and user mailing lists, or those that are simply overwhelmed by the traffic.</p>

<h3 id="flink-graduation">Flink graduation</h3>

<p>The biggest news is that the Apache board approved Flink as a top-level Apache project! The Flink team is working closely with the Apache press team for an official announcement, so stay tuned for details!</p>

<h3 id="new-flink-website">New Flink website</h3>

<p>The <a href="http://flink.apache.org">Flink website</a> got a total make-over, both in terms of appearance and content.</p>

<h3 id="flink-irc-channel">Flink IRC channel</h3>

<p>A new IRC channel called #flink was created at irc.freenode.org. An easy way to access the IRC channel is through the <a href="http://webchat.freenode.net/">web client</a>.  Feel free to stop by to ask anything or share your ideas about Apache Flink!</p>

<h3 id="meetups-and-talks">Meetups and Talks</h3>

<p>Apache Flink was presented in the <a href="http://www.meetup.com/Netherlands-Hadoop-User-Group/events/218635152">Amsterdam Hadoop User Group</a></p>

<h2 id="notable-code-contributions">Notable code contributions</h2>

<p><strong>Note:</strong> Code contributions listed here may not be part of a release or even the current snapshot yet.</p>

<h3 id="streaming-scala-api"><a href="https://github.com/apache/incubator-flink/pull/275">Streaming Scala API</a></h3>

<p>The Flink Streaming Java API recently got its Scala counterpart. Once merged, Flink Streaming users can use both Scala and Java for their development. The Flink Streaming Scala API is built as a thin layer on top of the Java API, making sure that the APIs are kept easily in sync.</p>

<h3 id="intermediate-datasets"><a href="https://github.com/apache/incubator-flink/pull/254">Intermediate datasets</a></h3>

<p>This pull request introduces a major change in the Flink runtime. Currently, the Flink runtime is based on the notion of operators that exchange data through channels. With the PR, intermediate data sets that are produced by operators become first-class citizens in the runtime. While this does not have any user-facing impact yet, it lays the groundwork for a slew of future features such as blocking execution, fine-grained fault-tolerance, and more efficient data sharing between cluster and client.</p>

<h3 id="configurable-execution-mode"><a href="https://github.com/apache/incubator-flink/pull/259">Configurable execution mode</a></h3>

<p>This pull request allows the user to change the object-reuse behaviour. Before this pull request, some operations would reuse objects passed to the user function while others would always create new objects. This introduces a system wide switch and changes all operators to either reuse objects or don’t reuse objects.</p>

<h3 id="distributed-coordination-via-akka"><a href="https://github.com/apache/incubator-flink/pull/149">Distributed Coordination via Akka</a></h3>

<p>Another major change is a complete rewrite of the JobManager / TaskManager components in Scala. In addition to that, the old RPC service was replaced by Actors, using the Akka framework.</p>

<h3 id="sorting-of-very-large-records"><a href="https://github.com/apache/incubator-flink/pull/249">Sorting of very large records</a></h3>

<p>Flink&#39;s internal sort-algorithms were improved to better handle large records (multiple 100s of megabytes or larger). Previously, the system did in some cases hold instances of multiple large records, resulting in high memory consumption and JVM heap thrashing. Through this fix, large records are streamed through the operators, reducing the memory consumption and GC pressure. The system now requires much less memory to support algorithms that work on such large records.</p>

<h3 id="kryo-serialization-as-the-new-default-fallback"><a href="https://github.com/apache/incubator-flink/pull/271">Kryo Serialization as the new default fallback</a></h3>

<p>Flink’s build-in type serialization framework is handles all common types very efficiently. Prior versions uses Avro to serialize types that the built-in framework could not handle.
Flink serialization system improved a lot over time and by now surpasses the capabilities of Avro in many cases. Kryo now serves as the default fallback serialization framework, supporting a much broader range of types.</p>

<h3 id="hadoop-filesystem-support"><a href="https://github.com/apache/incubator-flink/pull/268">Hadoop FileSystem support</a></h3>

<p>This change permits users to use all file systems supported by Hadoop with Flink. In practice this means that users can use Flink with Tachyon, Google Cloud Storage (also out of the box Flink YARN support on Google Compute Cloud), FTP and all the other file system implementations for Hadoop.</p>

<h2 id="heading-to-the-0.8.0-release">Heading to the 0.8.0 release</h2>

<p>The community is working hard together with the Apache infra team to migrate the Flink infrastructure to a top-level project. At the same time, the Flink community is working on the Flink 0.8.0 release which should be out very soon.</p>
</div>
				<a href="/news/2015/01/06/december-in-flink.html#disqus_thread">December 2014 in the Flink community</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/11/18/hadoop-compatibility.html">Hadoop Compatibility in Flink</a></h2>
				<p class="meta">18 Nov 2014</p>

				<div><p><a href="http://hadoop.apache.org">Apache Hadoop</a> is an industry standard for scalable analytical data processing. Many data analysis applications have been implemented as Hadoop MapReduce jobs and run in clusters around the world. Apache Flink can be an alternative to MapReduce and improves it in many dimensions. Among other features, Flink provides much better performance and offers APIs in Java and Scala, which are very easy to use. Similar to Hadoop, Flink’s APIs provide interfaces for Mapper and Reducer functions, as well as Input- and OutputFormats along with many more operators. While being conceptually equivalent, Hadoop’s MapReduce and Flink’s interfaces for these functions are unfortunately not source compatible.</p>

<h2 id="flink’s-hadoop-compatibility-package">Flink’s Hadoop Compatibility Package</h2>

<p><center>
<img src="/img/blog/hcompat-logos.png" style="width:30%;margin:15px">
</center></p>

<p>To close this gap, Flink provides a Hadoop Compatibility package to wrap functions implemented against Hadoop’s MapReduce interfaces and embed them in Flink programs. This package was developed as part of a <a href="https://developers.google.com/open-source/soc/">Google Summer of Code</a> 2014 project. </p>

<p>With the Hadoop Compatibility package, you can reuse all your Hadoop</p>

<ul>
<li><code>InputFormats</code> (mapred and mapreduce APIs)</li>
<li><code>OutputFormats</code> (mapred and mapreduce APIs)</li>
<li><code>Mappers</code> (mapred API)</li>
<li><code>Reducers</code> (mapred API)</li>
</ul>

<p>in Flink programs without changing a line of code. Moreover, Flink also natively supports all Hadoop data types (<code>Writables</code> and <code>WritableComparable</code>).</p>

<p>The following code snippet shows a simple Flink WordCount program that solely uses Hadoop data types, InputFormat, OutputFormat, Mapper, and Reducer functions. </p>
<div class="highlight"><pre><code class="language-java" data-lang="java"><span class="c1">// Definition of Hadoop Mapper function</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Tokenizer</span> <span class="kd">implements</span> <span class="n">Mapper</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>
<span class="c1">// Definition of Hadoop Reducer function</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">Counter</span> <span class="kd">implements</span> <span class="n">Reducer</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="o">{</span> <span class="o">...</span> <span class="o">}</span>

<span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
  <span class="kd">final</span> <span class="n">String</span> <span class="n">inputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">];</span>
  <span class="kd">final</span> <span class="n">String</span> <span class="n">outputPath</span> <span class="o">=</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">];</span>

  <span class="kd">final</span> <span class="n">ExecutionEnvironment</span> <span class="n">env</span> <span class="o">=</span> <span class="n">ExecutionEnvironment</span><span class="o">.</span><span class="na">getExecutionEnvironment</span><span class="o">();</span>

  <span class="c1">// Setup Hadoop’s TextInputFormat</span>
  <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;</span> <span class="n">hadoopInputFormat</span> <span class="o">=</span> 
      <span class="k">new</span> <span class="n">HadoopInputFormat</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;(</span>
        <span class="k">new</span> <span class="nf">TextInputFormat</span><span class="o">(),</span> <span class="n">LongWritable</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="n">Text</span><span class="o">.</span><span class="na">class</span><span class="o">,</span> <span class="k">new</span> <span class="nf">JobConf</span><span class="o">());</span>
  <span class="n">TextInputFormat</span><span class="o">.</span><span class="na">addInputPath</span><span class="o">(</span><span class="n">hadoopInputFormat</span><span class="o">.</span><span class="na">getJobConf</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">inputPath</span><span class="o">));</span>

  <span class="c1">// Read a DataSet with the Hadoop InputFormat</span>
  <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">&gt;&gt;</span> <span class="n">text</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="na">createInput</span><span class="o">(</span><span class="n">hadoopInputFormat</span><span class="o">);</span>
  <span class="n">DataSet</span><span class="o">&lt;</span><span class="n">Tuple2</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;&gt;</span> <span class="n">words</span> <span class="o">=</span> <span class="n">text</span>
    <span class="c1">// Wrap Tokenizer Mapper function</span>
    <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopMapFunction</span><span class="o">&lt;</span><span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span><span class="k">new</span> <span class="nf">Tokenizer</span><span class="o">()))</span>
    <span class="o">.</span><span class="na">groupBy</span><span class="o">(</span><span class="mi">0</span><span class="o">)</span>
    <span class="c1">// Wrap Counter Reducer function (used as Reducer and Combiner)</span>
    <span class="o">.</span><span class="na">reduceGroup</span><span class="o">(</span><span class="k">new</span> <span class="n">HadoopReduceCombineFunction</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">,</span> <span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
      <span class="k">new</span> <span class="nf">Counter</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Counter</span><span class="o">()));</span>

  <span class="c1">// Setup Hadoop’s TextOutputFormat</span>
  <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;</span> <span class="n">hadoopOutputFormat</span> <span class="o">=</span> 
    <span class="k">new</span> <span class="n">HadoopOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(</span>
      <span class="k">new</span> <span class="n">TextOutputFormat</span><span class="o">&lt;</span><span class="n">Text</span><span class="o">,</span> <span class="n">LongWritable</span><span class="o">&gt;(),</span> <span class="k">new</span> <span class="nf">JobConf</span><span class="o">());</span>
  <span class="n">hadoopOutputFormat</span><span class="o">.</span><span class="na">getJobConf</span><span class="o">().</span><span class="na">set</span><span class="o">(</span><span class="s">&quot;mapred.textoutputformat.separator&quot;</span><span class="o">,</span> <span class="s">&quot; &quot;</span><span class="o">);</span>
  <span class="n">TextOutputFormat</span><span class="o">.</span><span class="na">setOutputPath</span><span class="o">(</span><span class="n">hadoopOutputFormat</span><span class="o">.</span><span class="na">getJobConf</span><span class="o">(),</span> <span class="k">new</span> <span class="nf">Path</span><span class="o">(</span><span class="n">outputPath</span><span class="o">));</span>

  <span class="c1">// Output &amp; Execute</span>
  <span class="n">words</span><span class="o">.</span><span class="na">output</span><span class="o">(</span><span class="n">hadoopOutputFormat</span><span class="o">);</span>
  <span class="n">env</span><span class="o">.</span><span class="na">execute</span><span class="o">(</span><span class="s">&quot;Hadoop Compat WordCount&quot;</span><span class="o">);</span>
<span class="o">}</span>
</code></pre></div>
<p>As you can see, Flink represents Hadoop key-value pairs as <code>Tuple2&lt;key, value&gt;</code> tuples. Note, that the program uses Flink’s <code>groupBy()</code> transformation to group data on the key field (field 0 of the <code>Tuple2&lt;key, value&gt;</code>) before it is given to the Reducer function. At the moment, the compatibility package does not evaluate custom Hadoop partitioners, sorting comparators, or grouping comparators.</p>

<p>Hadoop functions can be used at any position within a Flink program and of course also be mixed with native Flink functions. This means that instead of assembling a workflow of Hadoop jobs in an external driver method or using a workflow scheduler such as <a href="http://oozie.apache.org">Apache Oozie</a>, you can implement an arbitrary complex Flink program consisting of multiple Hadoop Input- and OutputFormats, Mapper and Reducer functions. When executing such a Flink program, data will be pipelined between your Hadoop functions and will not be written to HDFS just for the purpose of data exchange.</p>

<p><center>
<img src="/img/blog/hcompat-flow.png" style="width:100%;margin:15px">
</center></p>

<h2 id="what-comes-next?">What comes next?</h2>

<p>While the Hadoop compatibility package is already very useful, we are currently working on a dedicated Hadoop Job operation to embed and execute Hadoop jobs as a whole in Flink programs, including their custom partitioning, sorting, and grouping code. With this feature, you will be able to chain multiple Hadoop jobs, mix them with Flink functions, and other operations such as <a href="/docs/0.7-incubating/spargel_guide.html">Spargel</a> operations (Pregel/Giraph-style jobs).</p>

<h2 id="summary">Summary</h2>

<p>Flink lets you reuse a lot of the code you wrote for Hadoop MapReduce, including all data types, all Input- and OutputFormats, and Mapper and Reducers of the mapred-API. Hadoop functions can be used within Flink programs and mixed with all other Flink functions. Due to Flink’s pipelined execution, Hadoop functions can arbitrarily be assembled without data exchange via HDFS. Moreover, the Flink community is currently working on a dedicated Hadoop Job operation to supporting the execution of Hadoop jobs as a whole.</p>

<p>If you want to use Flink’s Hadoop compatibility package checkout our <a href="/docs/0.7-incubating/hadoop_compatibility.html">documentation</a>.</p>

<p><br>
<small>Written by Fabian Hueske (<a href="https://twitter.com/fhueske">@fhueske</a>).</small></p>
</div>
				<a href="/news/2014/11/18/hadoop-compatibility.html#disqus_thread">Hadoop Compatibility in Flink</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/11/04/release-0.7.0.html">Apache Flink 0.7.0 available</a></h2>
				<p class="meta">04 Nov 2014</p>

				<div><p>We are pleased to announce the availability of Flink 0.7.0. This release includes new user-facing features as well as performance and bug fixes, brings the Scala and Java APIs in sync, and introduces Flink Streaming. A total of 34 people have contributed to this release, a big thanks to all of them!</p>

<p>Download Flink 0.7.0 <a href="http://flink.incubator.apache.org/downloads.html">here</a></p>

<p>See the release changelog <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12327648">here</a></p>

<h2 id="overview-of-major-new-features">Overview of major new features</h2>

<p><strong>Flink Streaming:</strong> The gem of the 0.7.0 release is undoubtedly Flink Streaming. Available currently in alpha, Flink Streaming provides a Java API on top of Apache Flink that can consume streaming data sources (e.g., from Apache Kafka, Apache Flume, and others) and process them in real time. A dedicated blog post on Flink Streaming and its performance is coming up here soon. You can check out the Streaming programming guide <a href="http://flink.incubator.apache.org/docs/0.7-incubating/streaming_guide.html">here</a>.</p>

<p><strong>New Scala API:</strong> The Scala API has been completely rewritten. The Java and Scala APIs have now the same syntax and transformations and will be kept from now on in sync in every future release. See the new Scala API <a href="http://flink.incubator.apache.org/docs/0.7-incubating/programming_guide.html">here</a>.</p>

<p><strong>Logical key expressions:</strong> You can now specify grouping and joining keys with logical names for member variables of POJO data types. For example, you can join two data sets as <code>persons.join(cities).where(“zip”).equalTo(“zipcode”)</code>. Read more <a href="http://flink.incubator.apache.org/docs/0.7-incubating/programming_guide.html#specifying-keys">here</a>.</p>

<p><strong>Hadoop MapReduce compatibility:</strong> You can run unmodified Hadoop Mappers and Reducers (mapred API) in Flink, use all Hadoop data types, and read data with all Hadoop InputFormats.</p>

<p><strong>Collection-based execution backend:</strong> The collection-based execution backend enables you to execute a Flink job as a simple Java collections program, bypassing completely the Flink runtime and optimizer. This feature is extremely useful for prototyping, and embedding Flink jobs in projects in a very lightweight manner.</p>

<p><strong>Record API deprecated:</strong> The (old) Stratosphere Record API has been marked as deprecated and is planned for removal in the 0.9.0 release.</p>

<p><strong>BLOB service:</strong> This release contains a new service to distribute jar files and other binary data among the JobManager, TaskManagers and the client. </p>

<p><strong>Intermediate data sets:</strong> A major rewrite of the system internals introduces intermediate data sets as first class citizens. The internal state machine that tracks the distributed tasks has also been completely rewritten for scalability. While this is not visible as a user-facing feature yet, it is the foundation for several upcoming exciting features.</p>

<p><strong>Note:</strong> Currently, there is limited support for Java 8 lambdas when compiling and running from an IDE. The problem is due to type erasure and whether Java compilers retain type information. We are currently working with the Eclipse and OpenJDK communities to resolve this.</p>

<h2 id="contributors">Contributors</h2>

<ul>
<li>Tamas Ambrus</li>
<li>Mariem Ayadi</li>
<li>Marton Balassi</li>
<li>Daniel Bali</li>
<li>Ufuk Celebi</li>
<li>Hung Chang</li>
<li>David Eszes</li>
<li>Stephan Ewen</li>
<li>Judit Feher</li>
<li>Gyula Fora</li>
<li>Gabor Hermann</li>
<li>Fabian Hueske</li>
<li>Vasiliki Kalavri</li>
<li>Kristof Kovacs</li>
<li>Aljoscha Krettek</li>
<li>Sebastian Kruse</li>
<li>Sebastian Kunert</li>
<li>Matyas Manninger</li>
<li>Robert Metzger</li>
<li>Mingliang Qi</li>
<li>Till Rohrmann</li>
<li>Henry Saputra</li>
<li>Chesnay Schelper</li>
<li>Moritz Schubotz</li>
<li>Hung Sendoh Chang</li>
<li>Peter Szabo</li>
<li>Jonas Traub</li>
<li>Fabian Tschirschnitz</li>
<li>Artem Tsikiridis</li>
<li>Kostas Tzoumas</li>
<li>Timo Walther</li>
<li>Daniel Warneke</li>
<li>Tobias Wiens</li>
<li>Yingjun Wu</li>
</ul>
</div>
				<a href="/news/2014/11/04/release-0.7.0.html#disqus_thread">Apache Flink 0.7.0 available</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/10/03/upcoming_events.html">Upcoming Events</a></h2>
				<p class="meta">03 Oct 2014</p>

				<div><p>We are happy to announce several upcoming Flink events both in Europe and the US. Starting with a <strong>Flink hackathon in Stockholm</strong> (Oct 8-9) and a talk about Flink at the <strong>Stockholm Hadoop User Group</strong> (Oct 8). This is followed by the very first <strong>Flink Meetup in Berlin</strong> (Oct 15). In the US, there will be two Flink Meetup talks: the first one at the <strong>Pasadena Big Data User Group</strong> (Oct 29) and the second one at <strong>Silicon Valley Hands On Programming Events</strong> (Nov 4).</p>

<p>We are looking forward to seeing you at any of these events. The following is an overview of each event and links to the respective Meetup pages.</p>

<h3 id="flink-hackathon,-stockholm-(oct-8-9)">Flink Hackathon, Stockholm (Oct 8-9)</h3>

<p>The hackathon will take place at KTH/SICS from Oct 8th-9th. You can sign up here: <a href="https://docs.google.com/spreadsheet/viewform?formkey=dDZnMlRtZHJ3Z0hVTlFZVjU2MWtoX0E6MA">https://docs.google.com/spreadsheet/viewform?formkey=dDZnMlRtZHJ3Z0hVTlFZVjU2MWtoX0E6MA</a>.</p>

<p>Here is a rough agenda and a list of topics to work upon or look into. Suggestions and more topics are welcome.</p>

<h4 id="wednesday-(8th)">Wednesday (8th)</h4>

<p>9:00 - 10:00  Introduction to Apache Flink, System overview, and Dev
environment (by Stephan)</p>

<p>10:15 - 11:00 Introduction to the topics (Streaming API and system by Gyula
&amp; Marton), (Graphs by Vasia / Martin / Stephan)</p>

<p>11:00 - 12:30 Happy hacking (part 1)</p>

<p>12:30 - Lunch (Food will be provided by KTH / SICS. A big thank you to them
and also to Paris, for organizing that)</p>

<p>13:xx - Happy hacking (part 2)</p>

<h4 id="thursday-(9th)">Thursday (9th)</h4>

<p>Happy hacking (continued)</p>

<h4 id="suggestions-for-topics">Suggestions for topics</h4>

<h5 id="streaming">Streaming</h5>

<ul>
<li><p>Sample streaming applications (e.g. continuous heavy hitters and topics
on the twitter stream)</p></li>
<li><p>Implement a simple SQL to Streaming program parser. Possibly using
Apache Calcite (<a href="http://optiq.incubator.apache.org/">http://optiq.incubator.apache.org/</a>)</p></li>
<li><p>Implement different windowing methods (count-based, time-based, ...)</p></li>
<li><p>Implement different windowed operations (windowed-stream-join,
windowed-stream-co-group)</p></li>
<li><p>Streaming state, and interaction with other programs (that access state
of a stream program)</p></li>
</ul>

<h5 id="graph-analysis">Graph Analysis</h5>

<ul>
<li><p>Prototype a Graph DSL (simple graph building, filters, graph
properties, some algorithms)</p></li>
<li><p>Prototype abstractions different Graph processing paradigms
(vertex-centric, partition-centric).</p></li>
<li><p>Generalize the delta iterations, allow flexible state access.</p></li>
</ul>

<h3 id="meetup:-hadoop-user-group-talk,-stockholm-(oct-8)">Meetup: Hadoop User Group Talk, Stockholm (Oct 8)</h3>

<p>Hosted by Spotify, opens at 6 PM.</p>

<p><a href="http://www.meetup.com/stockholm-hug/events/207323222/">http://www.meetup.com/stockholm-hug/events/207323222/</a></p>

<h3 id="1st-flink-meetup,-berlin-(oct-15)">1st Flink Meetup, Berlin (Oct 15)</h3>

<p>We are happy to announce the first Flink meetup in Berlin. You are very welcome to to sign up and attend. The event will be held in Betahaus Cafe.</p>

<p><a href="http://www.meetup.com/Apache-Flink-Meetup/events/208227422/">http://www.meetup.com/Apache-Flink-Meetup/events/208227422/</a></p>

<h3 id="meetup:-pasadena-big-data-user-group-(oct-29)">Meetup: Pasadena Big Data User Group (Oct 29)</h3>

<p><a href="http://www.meetup.com/Pasadena-Big-Data-Users-Group/">http://www.meetup.com/Pasadena-Big-Data-Users-Group/</a></p>

<h3 id="meetup:-silicon-valley-hands-on-programming-events-(nov-4)">Meetup: Silicon Valley Hands On Programming Events (Nov 4)</h3>

<p><a href="http://www.meetup.com/HandsOnProgrammingEvents/events/210504392/">http://www.meetup.com/HandsOnProgrammingEvents/events/210504392/</a></p>
</div>
				<a href="/news/2014/10/03/upcoming_events.html#disqus_thread">Upcoming Events</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/09/26/release-0.6.1.html">Apache Flink 0.6.1 available</a></h2>
				<p class="meta">26 Sep 2014</p>

				<div><p>We are happy to announce the availability of Flink 0.6.1.</p>

<p>0.6.1 is a maintenance release, which includes minor fixes across several parts
of the system. We suggest all users of Flink to work with this newest version.</p>

<p><a href="/downloads.html">Download</a> the release today.</p>
</div>
				<a href="/news/2014/09/26/release-0.6.1.html#disqus_thread">Apache Flink 0.6.1 available</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/08/26/release-0.6.html">Apache Flink 0.6 available</a></h2>
				<p class="meta">26 Aug 2014</p>

				<div><p>We are happy to announce the availability of Flink 0.6. This is the
first release of the system inside the Apache Incubator and under the
name Flink. Releases up to 0.5 were under the name Stratosphere, the
academic and open source project that Flink originates from.</p>

<h2 id="what-is-flink?">What is Flink?</h2>

<p>Apache Flink is a general-purpose data processing engine for
clusters. It runs on YARN clusters on top of data stored in Hadoop, as
well as stand-alone. Flink currently has programming APIs in Java and
Scala. Jobs are executed via Flink&#39;s own runtime engine. Flink
features:</p>

<p><strong>Robust in-memory and out-of-core processing:</strong> once read, data stays
  in memory as much as possible, and is gracefully de-staged to disk in
  the presence of memory pressure from limited memory or other
  applications. The runtime is designed to perform very well both in
  setups with abundant memory and in setups where memory is scarce.</p>

<p><strong>POJO-based APIs:</strong> when programming, you do not have to pack your
  data into key-value pairs or some other framework-specific data
  model. Rather, you can use arbitrary Java and Scala types to model
  your data.</p>

<p><strong>Efficient iterative processing:</strong> Flink contains explicit &quot;iterate&quot; operators
  that enable very efficient loops over data sets, e.g., for machine
  learning and graph applications.</p>

<p><strong>A modular system stack:</strong> Flink is not a direct implementation of its
  APIs but a layered system. All programming APIs are translated to an
  intermediate program representation that is compiled and optimized
  via a cost-based optimizer. Lower-level layers of Flink also expose
  programming APIs for extending the system.</p>

<p><strong>Data pipelining/streaming:</strong> Flink&#39;s runtime is designed as a
  pipelined data processing engine rather than a batch processing
  engine. Operators do not wait for their predecessors to finish in
  order to start processing data. This results to very efficient
  handling of large data sets.</p>

<h2 id="release-0.6">Release 0.6</h2>

<p>Flink 0.6 builds on the latest Stratosphere 0.5 release. It includes
many bug fixes and improvements that make the system more stable and
robust, as well as breaking API changes.</p>

<p>The full release notes are available <a href="https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315522&amp;version=12327101">here</a>.</p>

<p>Download the release <a href="http://flink.incubator.apache.org/downloads.html">here</a>.</p>

<h2 id="contributors">Contributors</h2>

<ul>
<li>Wilson Cao</li>
<li>Ufuk Celebi</li>
<li>Stephan Ewen</li>
<li>Jonathan Hasenburg</li>
<li>Markus Holzemer</li>
<li>Fabian Hueske</li>
<li>Sebastian Kunert</li>
<li>Vikhyat Korrapati</li>
<li>Aljoscha Krettek</li>
<li>Sebastian Kruse</li>
<li>Raymond Liu</li>
<li>Robert Metzger</li>
<li>Mingliang Qi</li>
<li>Till Rohrmann</li>
<li>Henry Saputra</li>
<li>Chesnay Schepler</li>
<li>Kostas Tzoumas</li>
<li>Robert Waury</li>
<li>Timo Walther</li>
<li>Daniel Warneke</li>
<li>Tobias Wiens</li>
</ul>
</div>
				<a href="/news/2014/08/26/release-0.6.html#disqus_thread">Apache Flink 0.6 available</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/05/31/release-0.5.html">Stratosphere version 0.5 available</a></h2>
				<p class="meta">31 May 2014</p>

				<div><p>We are happy to announce a new major Stratosphere release, version 0.5. This release adds many new features and improves the interoperability, stability, and performance of the system. The major theme of the release is the completely new Java API that makes it easy to write powerful distributed programs.</p>

<p>The release can be downloaded from the <a href="http://stratosphere.eu/downloads/">Stratosphere website</a> and from <a href="https://github.com/stratosphere/stratosphere/releases/tag/release-0.5">GitHub</a>. All components are available as Apache Maven dependencies, making it simple to include Stratosphere in other projects. The website provides <a href="http://stratosphere.eu/docs/0.5/">extensive documentation</a> of the system and the new features.</p>

<h2 id="shortlist-of-new-features">Shortlist of new Features</h2>

<p>Below is a short list of the most important additions to the Stratosphere system.</p>

<h4 id="new-java-api">New Java API</h4>

<p>This release introduces a completely new <strong>data set-centric Java API</strong>. This programming model significantly eases the development of Stratosphere programs, supports flexible use of regular Java classes as data types, and adds many new built-in operators to simplify the writing of powerful programs. The result are programs that need less code, are more readable, interoperate better with existing code, and execute faster.</p>

<p>Take a look at the <a href="http://stratosphere.eu/docs/0.5/programming_guides/examples_java.html">examples</a>  to get a feel for the API.</p>

<h4 id="general-api-improvements">General API Improvements</h4>

<p><strong>Broadcast Variables:</strong> Publish a data set to all instances of another operator. This is handy if the your operator depends on the result of a computation, e.g., filter all values smaller than the average.</p>

<p><strong>Distributed Cache:</strong> Make (local and HDFS) files locally available on each machine processing a task.</p>

<p><strong>Iteration Termination Improvements</strong> Iterative algorithms can now terminate based on intermediate data sets, not only through aggregated statistics.</p>

<p><strong>Collection data sources and sinks:</strong> Speed-up the development and testing of Stratosphere programs by reading data from regular Java collections and inserting back into them.</p>

<p><strong>JDBC data sources and sinks:</strong> Read data from and write data to relational databases using a JDBC driver.</p>

<p><strong>Hadoop input format and output format support:</strong> Read and write data with any Hadoop input or output format.</p>

<p><strong>Support for Avro encoded data:</strong> Read data that has been materialized using Avro.</p>

<p><strong>Deflate Files:</strong> Stratosphere now transparently reads <code>.deflate</code> compressed files.</p>

<h4 id="runtime-and-optimizer-improvements">Runtime and Optimizer Improvements</h4>

<p><strong>DAG Runtime Streaming:</strong> Detection and resolution of streaming data flow deadlocks in the data flow optimizer.</p>

<p><strong>Intermediate results across iteration boundaries:</strong> Intermediate results computed outside iterative parts can be used inside iterative parts of the program.</p>

<p><strong>Stability fixes:</strong> Various stability fixes in both optimizer and runtime.</p>

<h4 id="setup-&amp;-tooling">Setup &amp; Tooling</h4>

<p><strong>Improved YARN support:</strong> Many improvements based on user-feedback: Packaging, Permissions, Error handling.</p>

<p><strong>Java 8 compatibility</strong></p>

<h2 id="contributors">Contributors</h2>

<p>In total, 26 people have contributed to Stratosphere since the last release. Thank you for making this project possible!</p>

<ul>
<li>Alexander Alexandrov</li>
<li>Jesus Camacho</li>
<li>Ufuk Celebi</li>
<li>Mikhail Erofeev</li>
<li>Stephan Ewen</li>
<li>Alexandr Ferodov</li>
<li>Filip Haase</li>
<li>Jonathan Hasenberg</li>
<li>Markus Holzemer</li>
<li>Fabian Hueske</li>
<li>Vasia Kalavri</li>
<li>Aljoscha Krettek</li>
<li>Rajika Kumarasiri</li>
<li>Sebastian Kunert</li>
<li>Aaron Lam</li>
<li>Robert Metzger</li>
<li>Faisal Moeen</li>
<li>Martin Neumann</li>
<li>Mingliang Qi</li>
<li>Till Rohrmann</li>
<li>Chesnay Schepler</li>
<li>Vyachislav Soludev</li>
<li>Tuan Trieu</li>
<li>Artem Tsikiridis</li>
<li>Timo Walther</li>
<li>Robert Waury</li>
</ul>

<h2 id="stratosphere-is-going-apache">Stratosphere is going Apache</h2>

<p>The Stratosphere project has been accepted to the Apache Incubator and will continue its work under the umbrella of the Apache Software Foundation. Due to a name conflict, we are switching the name of the project. We will make future releases of Stratosphere through the Apache foundation under a new name.</p>
</div>
				<a href="/news/2014/05/31/release-0.5.html#disqus_thread">Stratosphere version 0.5 available</a>
			</article>
			
			<article>
				<h2><a href="/news/2014/04/16/stratosphere-goes-apache-incubator.html">Stratosphere accepted as Apache Incubator Project</a></h2>
				<p class="meta">16 Apr 2014</p>

				<div><p>We are happy to announce that Stratosphere has been accepted as a project for the <a href="https://incubator.apache.org/">Apache Incubator</a>. The <a href="https://wiki.apache.org/incubator/StratosphereProposal">proposal</a> has been accepted by the Incubator PMC members earlier this week. The Apache Incubator is the first step in the process of giving a project to the <a href="http://apache.org">Apache Software Foundation</a>. While under incubation, the project will move to the Apache infrastructure and adopt the community-driven development principles of the Apache Foundation. Projects can graduate from incubation to become top-level projects if they show activity, a healthy community dynamic, and releases.</p>

<p>We are glad to have Alan Gates as champion on board, as well as a set of great mentors, including Sean Owen, Ted Dunning, Owen O&#39;Malley, Henry Saputra, and Ashutosh Chauhan. We are confident that we will make this a great open source effort.</p>
</div>
				<a href="/news/2014/04/16/stratosphere-goes-apache-incubator.html#disqus_thread">Stratosphere accepted as Apache Incubator Project</a>
			</article>
			
		</div>
		<div class="col-md-2"></div>
	</div>
</div>

<script type="text/javascript">
/* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
var disqus_shortname = 'stratosphere-eu'; // required: replace example with your forum shortname

/* * * DON'T EDIT BELOW THIS LINE * * */
(function () {
    var s = document.createElement('script'); s.async = true;
    s.type = 'text/javascript';
    s.src = '//' + disqus_shortname + '.disqus.com/count.js';
    (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
}());
</script>



<!-- Pagination links -->
<ul class="pager">
	<li>
	
		<span>Previous</span>
	
	</li>
	<li>
		<span class="page_number ">Page: 1 of 3</span>
	</li>
	<li>
	
		<a href="/blog/page2" class="next">Next</a>
	
	</li>
</ul>


    </div>
    <!--<section id="af-upfooter" class="af-section">
	<div class="container">
		<p>Apache Flink is an effort undergoing incubation at The Apache
			Software Foundation (ASF), sponsored by the Apache Incubator PMC.
			Incubation is required of all newly accepted projects until a further
			review indicates that the infrastructure, communications, and
			decision making process have stabilized in a manner consistent with
			other successful ASF projects. While incubation status is not
			necessarily a reflection of the completeness or stability of the
			code, it does indicate that the project has yet to be fully endorsed
			by the ASF.</p>
		<a href="http://incubator.apache.org"> <img class="img-responsive"
			src="/img/main/apache-incubator-logo.png" alt="Apache Flink" />
		</a>
		<p class="text-center">
			<a href="/privacy-policy.html" title="Privacy Policy"
				class="af-privacy-policy">Privacy Policy</a>
		</p>
	</div>
</section>-->

<footer id="af-footer">
	<div class="container">
		<div class="row">
			<div class="col-md-3">
				<h3>Documentation</h3>
				<ul class="af-footer-menu">

					<li><a href="/docs/0.8/">0.8.0</a></li>
					<li><a href="/docs/0.8/api/java/">0.8.0 Javadocs</a></li>
					<li><a
						href="/docs/0.8/api/scala/index.html#org.apache.flink.api.scala.package">0.8.0 Scaladocs</a></li>
				</ul>
			</div>
			<div class="col-md-3">
				<h3>Community</h3>
				<ul class="af-footer-menu">
					<li><a href="/community.html#mailing-lists">Mailing Lists</a></li>
					<li><a href="https://issues.apache.org/jira/browse/FLINK"
						target="blank">Issues <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a href="/community.html#team">Team</a></li>
					<li><a href="/how-to-contribute.html">How to contribute</a></li>
					<li><a href="/coding_guidelines.html">Coding Guidelines</a></li>
				</ul>
			</div>
			<div class="col-md-3">
				<h3>ASF</h3>
				<ul class="af-footer-menu">
					<li><a href="http://www.apache.org/" target="blank">Apache
							Software foundation <span class="glyphicon glyphicon-new-window"></span>
					</a></li>
					<li><a
						href="http://www.apache.org/foundation/how-it-works.html"
						target="blank">How it works <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a href="http://www.apache.org/foundation/thanks.html"
						target="blank">Thanks <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a
						href="http://www.apache.org/foundation/sponsorship.html"
						target="blank">Become a sponsor <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a href="http://incubator.apache.org/projects/flink.html"
						target="blank">Incubation status page <span
							class="glyphicon glyphicon-new-window"></span></a></li>
				</ul>
			</div>
			<div class="col-md-3">
				<h3>Project</h3>
				<ul class="af-footer-menu">
					<li><a href="/material.html" target="blank">Material <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a
						href="https://cwiki.apache.org/confluence/display/FLINK"
						target="blank">Wiki <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a
						href="https://wiki.apache.org/incubator/StratosphereProposal"
						target="blank">Incubator proposal <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a href="http://www.apache.org/licenses/LICENSE-2.0"
						target="blank">License <span
							class="glyphicon glyphicon-new-window"></span></a></li>
					<li><a href="https://github.com/apache/incubator-flink"
						target="blank">Source code <span
							class="glyphicon glyphicon-new-window"></span></a></li>
				</ul>
			</div>
		</div>
	</div>
	<div class="af-footer-bar">
		<div class="container">
		  <p>Copyright &copy 2014-2015, <a href="http://www.apache.org">The Apache Software Foundation</a>. All Rights Reserved. Apache and the Apache feather logo are trademarks of the Apache Software Foundation.
                  </p>
                  <div>
                    <div style="float:left">
                      <p>
                        <a href="/privacy-policy.html" title="Privacy Policy" class="af-privacy-policy">Privacy Policy</a>
                    </p>
                    </div>
                    <div style="float:right">
                    <p>
                      <a href="/blog/feed.xml" class="af-privacy-policy">RSS Feed</a>
                    </p>
                    </div>
                   </div>
    		</div>
	</div>
</footer>

    <!-- Google Analytics -->
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-52545728-1', 'auto');
      ga('send', 'pageview');
    </script>
    <script src="/js/main/jquery.mobile.events.min.js"></script>
    <script src="/js/main/main.js"></script>
  </body>
</html>
